# mapper.py
import os
import grpc
import sys
import random
from reducer_pb2 import ReduceRequest, KeyValuePair
from reducer_pb2_grpc import ReducerStub

class Mapper:
    def __init__(self, mapper_id, input_data, centroids):
        self.mapper_id = mapper_id
        self.input_data = input_data
        self.centroids = centroids
        self.channel = grpc.insecure_channel('localhost:50051')  # Assuming reducer runs on localhost

    def read_input_data(self, input_indices):
        # Read only the indices assigned to this mapper
        data_points = []

        if os.path.isfile(self.input_data):
            # Scenario 1: Input data contains only one big file
            with open(self.input_data) as f:
                for i, line in enumerate(f):
                    if i in input_indices:
                        data_points.append([float(x) for x in line.strip().split()])

        elif os.path.isdir(self.input_data):
            # Scenario 2: Input data contains multiple files
            files = os.listdir(self.input_data)
            for i, filename in enumerate(files):
                if i in input_indices:
                    filepath = os.path.join(self.input_data, filename)
                    with open(filepath) as f:
                        for line in f:
                            data_points.append([float(x) for x in line.strip().split()])

        return data_points

    def map_function(self, data_point):
        """
        Applies the Map function to each data point to generate intermediate key-value pairs.

        Args:
            data_point (list): A single data point.

        Returns:
            list: List containing a single intermediate key-value pair.
        """
        nearest_centroid_index = self.find_nearest_centroid(data_point)
        return [KeyValuePair(key=nearest_centroid_index, value=data_point)]

    def find_nearest_centroid(self, data_point):
        """
        Finds the index of the nearest centroid to a given data point.

        Args:
            data_point (list): Coordinates of the data point.

        Returns:
            int: Index of the nearest centroid.
        """
        min_distance = float('inf')
        nearest_centroid_index = None

        # Calculate distance from data point to each centroid
        for i, centroid in enumerate(self.centroids):
            distance = self.euclidean_distance(data_point, centroid)
            if distance < min_distance:
                min_distance = distance
                nearest_centroid_index = i

        return nearest_centroid_index

    def euclidean_distance(self, point1, point2):
        """
        Computes the Euclidean distance between two points.

        Args:
            point1 (list): Coordinates of the first point.
            point2 (list): Coordinates of the second point.

        Returns:
            float: Euclidean distance between the two points.
        """
        return sum((a - b) ** 2 for a, b in zip(point1, point2)) ** 0.5

    def partition(self, intermediate_pairs, num_reducers):
        """
        Partitions the output of the Map function into smaller partitions.

        Args:
            intermediate_pairs (list): List of intermediate key-value pairs generated by the Map function.
            num_reducers (int): Number of reducers.

        Returns:
            dict: Dictionary where keys are partition IDs and values are lists of key-value pairs.
        """
        partitions = {i: [] for i in range(num_reducers)}

        # Distribute key-value pairs to partitions based on simple partitioning logic (key % num_reducers)
        for key_value_pair in intermediate_pairs:
            partition_id = key_value_pair.key % num_reducers
            partitions[partition_id].append(key_value_pair)

        return partitions

    def write_to_file(self, partitioned_data):
        """
        Writes the partitioned data to files in the mapper's directory.

        Args:
            partitioned_data (dict): Partitioned data where keys are partition IDs and values are lists of key-value pairs.
        """
        for partition_id, data in partitioned_data.items():
            filename = f"partition_{self.mapper_id}_{partition_id}.txt"
            with open(filename, "w") as f:
                for key_value_pair in data:
                    f.write(f"{key_value_pair.key} {','.join(map(str, key_value_pair.value))}\n")

    def run(self, input_indices):
        # Read input data assigned to this mapper
        data_points = self.read_input_data(input_indices)

        # Apply Map function to each data point
        intermediate_pairs = []
        for data_point in data_points:
            intermediate_pairs.extend(self.map_function(data_point))

        # Partition the intermediate pairs
        partitioned_data = self.partition(intermediate_pairs, num_reducers=4)

        # Write partitioned data to files
        self.write_to_file(partitioned_data)

        # Send partitioned data to reducers using gRPC
        stub = ReducerStub(self.channel)
        for partition_id, data in partitioned_data.items():
            request = ReduceRequest(reducer_id=partition_id, intermediate_pairs=data)
            response = stub.ProcessIntermediate(request)

def main():
    mapper_id = int(sys.argv[1])  # Mapper ID passed as command-line argument
    input_data = sys.argv[2]  # Input data path passed as command-line argument
    centroids = [[random.uniform(0, 10) for _ in range(3)] for _ in range(5)]  # Placeholder centroids
    input_indices = []  # Placeholder for input indices assigned to this mapper

    # Your logic to get input indices from master based on mapper ID

    mapper = Mapper(mapper_id, input_data, centroids)
    mapper.run(input_indices)

if __name__ == "__main__":
    main()
